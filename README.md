# NLP Exercice 1

The goal of the first exercise is to implement skip-gram with negative-sampling from scratch. 

## Implementation : our thought process

Use the package manager [pip](https://pip.pypa.io/en/stable/) to install foobar.

```bash
pip install foobar
```

## Usage

```python
import foobar

foobar.pluralize('word') # returns 'words'
foobar.pluralize('goose') # returns 'geese'
foobar.singularize('phenomena') # returns 'phenomenon'
```

## Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

Please make sure to update tests as appropriate.

## Additional resources : web sites and papers

+ https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/ : code python et théorie
+ http://mediamining.univ-lyon2.fr/people/guille/word_embedding/skip_gram_with_negative_sampling.html : théorie en francais 
+ https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb : code
+ http://www.claudiobellei.com/2018/01/07/backprop-word2vec-python/ : code et théorie
+ https://github.com/deborausujono/word2vecpy: code
+ https://github.com/chrisjmccormick/word2vec_commented/blob/master/word2vec.c:code
+ http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ :theorie


